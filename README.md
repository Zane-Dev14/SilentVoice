
# SilentVoice ğŸ‘ğŸ—£ï¸  
**Bridging ASL and Spoken Language with AI**  


## ğŸ“– Project Overview  
SilentVoice is an AI-powered system that uses a laptop camera to interpret **American Sign Language (ASL)** gestures in real-time and converts them into audible speech. Built with a custom neural network for sign recognition and integrated text-to-speech synthesis.

---

## âœ¨ Features  
- **Real-time ASL Recognition**: Processes hand signs via webcam at 15-30 FPS  
- **Custom Neural Network**: Lightweight CNN architecture optimized for ASL alphabet  
- **Text-to-Speech**: Instant vocal output using pyttsx3 (offline) or Google TTS (online)  
- **Cross-Platform**: Works on Windows, macOS, and Linux  
- **Privacy-First**: No cloud dependencyâ€”all processing happens locally  

---

## ğŸ¥ Demo  

---

## âš™ï¸ Installation  

### Prerequisites  
- Python 3.8+  
- Webcam-enabled laptop  
- Recommended: NVIDIA GPU (CUDA support for faster inference)  

### Setup  
git clone https://github.com/Zane-Dev14/SilentVoice.git
cd SilentVoice
pip install -r requirements.txt

### Dependencies  
- OpenCV (Real-time camera processing)  
- TensorFlow/PyTorch (Neural Network framework)  
- MediaPipe (Hand landmark detection)  
- pyttsx3 (Text-to-speech)  

---

## ğŸš€ Usage  
1. **Launch the System**  

python src/main.py


2. **Calibrate Camera**  
- Position hands within frame boundaries shown on screen  

3. **Perform ASL Signs**  
- System displays recognized letters/words  
- Speech output activates automatically  

4. **Customize Settings**  

# Example: Set speech speed to 150 words/minute
python src/main.py --speech_rate 150


---

## ğŸ§  Technical Details  

### Neural Network Architecture  

Input (128x128 grayscale)  
â†“  
Conv2D (32 filters, ReLU) â†’ MaxPool  
â†“  
Conv2D (64 filters, ReLU) â†’ MaxPool  
â†“  
Flatten â†’ Dense (128 units, ReLU)  
â†“  
Output (29 units - ASL A-Z + space/del/other)  


### Training Process  
- **Dataset**: ASL Alphabet (35,000+ images) + custom recordings  
- **Preprocessing**: Grayscale conversion, background normalization  
- **Accuracy**: 94.7% on test set (letters A-Z)  

### Text-to-Speech Engine  

engine = pyttsx3.init()
engine.say("Hello World")  # Offline (default)
# OR
use_google_tts("Hello World")  # Online (higher quality)


---

## ğŸ“‚ Project Structure  
```
SilentVoice/
â”œâ”€â”€ data/               # Training datasets
â”œâ”€â”€ models/             # Pretrained NN weights
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ camera.py       # Webcam feed handler
â”‚   â”œâ”€â”€ nn_model.py     # Neural network implementation  
â”‚   â”œâ”€â”€ processor.py    # Frame processing pipeline
â”‚   â””â”€â”€ tts.py          # Speech synthesis
â”œâ”€â”€ utils/              # Helper scripts
â”œâ”€â”€ docs/               # Technical documentation
â””â”€â”€ tests/              # Unit/integration tests
```

---

## ğŸ¤ Contributing  
1. Fork the repository  
2. Create your feature branch: `git checkout -b feature/your-idea`  
3. Submit a pull request  

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.  

---

## ğŸ“œ License  
MIT License - See [LICENSE](LICENSE)  

---

## ğŸ™ Acknowledgments  
- ASL Alphabet Dataset (https://www.kaggle.com/datasets/grassknoted/asl-alphabet)  
- MediaPipe Hands model for real-time hand tracking  

---

## ğŸ“§ Contact  
Project Lead: Eric/Shaarav
Email: ericatkusa@gmail.com  
GitHub: [@Zane-Dev14](https://github.com/Zane-Dev14)  
```

---

**Next Steps**:  
1. Replace placeholder images/links with actual content  
2. Update contact info and contribution guidelines  
3. Add detailed installation instructions for CUDA (if using GPU)  
4. Include benchmark results and system requirements  
